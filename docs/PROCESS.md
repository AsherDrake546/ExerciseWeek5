1.	I built a site-wide search feature that navigates you to certain keywords in a page when clicking suggested words in the search dropdown.
2.	I found micro-iteration to be irritating. I was unable to get the AI to stick to the steps strictly so it would sometimes output a lot of code for me to review. It also broke big things in several places, making me have to repeat steps and at one point deleting the css file and forcing me to reupload it from the original week 2 version I used as a base.
3.	The self-review caught a lot of potential problems, like things possibly not displaying right, but there were still some massive problems the review didn’t catch, like js files not being properly referenced or the click navigation feature not working correctly within a page.
4.	I liked that Copilot Agent can directly run within a Github repository, but I did not like how much slower it was than Claude code on the command line was. Some steps took up to 10 minutes and it was especially frustrating when getting it to fix bugs that it caused, since the wait times kept adding up. I also found the system of opening new sections/tabs for agents from each task confusing and unintuitive.
5.	The self-review tended to fix places where it didn’t follow best practices before, mainly polishing the code and catching some bigger issues.
6.	I prefer the CLI workflow as in-browser seems much slower and the process of merging the branches for each time is less intuitive to me than having it work directly on the files and just manually creating backups between steps.
7.	I would likely use micro-iteration +  self-review for implementing smaller features, but for larger features it seems faster to start with a larger piece of code and then review it, since the self review of each small step took longer than doing some aspects of the feature manually probably would have taken me.
